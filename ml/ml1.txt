统计对象，统计目的，统计方法。
对象：数据 -> 提取特征 -> 抽象出模型 -> 预测/分析
    数据：数字，文字，图像，视频，音频，各个组合。
    统计学习的前提：同类数据有一定统计规律性；同类数据：英文文章；互联网网页；数据库中的数据。
    随机变量描述数据中的特征；概率分布描述数据的统计规律。
    统计中：变量/变量组表示数据。
    数据分为：连续变量和离散变量。
目的：略。
方法：基于数据构建概率统计模型。由监督学习，无监督学习，强化学习组成。
    假设数据都是独立同分布的！
    统计学习三要素：1.假设空间(模型) 2.评价标准(策略) 3.选取最优模型(算法)

基本分类：
    监督学习：本质是学习输入到输出的映射的统计规律
    输入空间，特征向量，特征空间，输出空间。模型定义在特征空间上。
    X输入变量，Y输出变量。x输入变量某个实例，y输出变量某个实例。x一般称为特征向量。样本对，样本，样本点(x,y)组成训练集T。
    根据输入变量X和输出变量Y的类型不同，分为回归问题，分类问题，标注问题。

TODO


感知机：二元线性分类模型。输入特征向量x，输出是-1和+1。判别模型。本质求出分离超平面。损失函数 -> 梯度下降 -> 感知机模型。
    f(x) = sign(w * x + b)
    sign(x) = +1, x>=0
            = -1, x<0
    假设空间：{f|f(x) = wx + b}
    策略：定义（经验）损失函数并将损失函数极小化
        1.误分类个数，求w,b。但是不可导舍去。
        2.误分类点到超平面的距离。点到平面的距离公式：1/||w|| * |w * x0 + b|。
            由于误分类点满足：-y(i) * (w * x(i) + b) > 0
            距离公式可以化为：-1/||w|| * y(i) * (w * x(i) + b) -> -1/||w||sigma(y(i) * (w * x(i) + b))
            不考虑1/||w|| -> 损失函数定义为：-sigma(y(i) * (w * x(i) + b))，i是误分类的样本。
    算法：梯度下降。误分类驱动。对w和b求导，然后对误分类的点更新w和b。直到没有误分类的点。
    证明算法的收敛性 TODO

    对偶性：