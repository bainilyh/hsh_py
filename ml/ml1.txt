统计对象，统计目的，统计方法。
对象：数据 -> 提取特征 -> 抽象出模型 -> 预测/分析
    数据：数字，文字，图像，视频，音频，各个组合。
    统计学习的前提：同类数据有一定统计规律性；同类数据：英文文章；互联网网页；数据库中的数据。
    随机变量描述数据中的特征；概率分布描述数据的统计规律。
    统计中：变量/变量组表示数据。
    数据分为：连续变量和离散变量。
目的：略。
方法：基于数据构建概率统计模型。由监督学习，无监督学习，强化学习组成。
    假设数据都是独立同分布的！
    统计学习三要素：1.假设空间(模型) 2.评价标准(策略) 3.选取最优模型(算法)

基本分类：
    监督学习：本质是学习输入到输出的映射的统计规律
    输入空间，特征向量，特征空间，输出空间。模型定义在特征空间上。
    X输入变量，Y输出变量。x输入变量某个实例，y输出变量某个实例。x一般称为特征向量。样本对，样本，样本点(x,y)组成训练集T。
    根据输入变量X和输出变量Y的类型不同，分为回归问题，分类问题，标注问题。

TODO 重要


感知机：二元线性分类模型。输入特征向量x，输出是-1和+1。判别模型。本质求出分离超平面。损失函数 -> 梯度下降 -> 感知机模型。
    f(x) = sign(w * x + b)
    sign(x) = +1, x>=0
            = -1, x<0
    假设空间：{f|f(x) = wx + b}
    策略：定义（经验）损失函数并将损失函数极小化
        1.误分类个数，求w,b。但是不可导舍去。
        2.误分类点到超平面的距离。点到平面的距离公式：1/||w|| * |w * x0 + b|。
            由于误分类点满足：-y(i) * (w * x(i) + b) > 0
            距离公式可以化为：-1/||w|| * y(i) * (w * x(i) + b) -> -1/||w||sigma(y(i) * (w * x(i) + b))
            不考虑1/||w|| -> 损失函数定义为：-sigma(y(i) * (w * x(i) + b))，i是误分类的样本。
    算法：梯度下降。误分类驱动。对w和b求导，然后对误分类的点更新w和b。直到没有误分类的点。
    证明算法的收敛性 TODO

    对偶性：TODO



K近邻法(k-NN): 分类与回归；一般是分类问题。输入特征向量x，输出类别y。本质通过k个实例点，通过多数表决法进行预测。
    没有显示的学习过程。主要通过数据集对特征空间进行划分。
    三要素：k值选择，度量距离，分类决策规则。
    过程：1.找到与x特征向量距离最近的k个特征向量 2.计算这k个特征向量所属分类进行多数表决。
    度量距离：欧氏距离；Lp距离；Minkowski距离。
        [sigma([x(i) - x(j)]**p)]**1/p; p=2，欧式距离；p=1，曼哈顿距离。这里xi - xj是元素级别运算
    k值选择：k越小模型越复杂（估计误差越小），对噪音容忍度低。k越大近似误差越大（TODO)。近似误差和估计误差的差别！！
        当k=1就是最近邻。
        交叉验证选择最优K。
    多数表决：略。

    KD树：k维，二叉树。
        构造平衡KD树：依次对k维中的每一维根据这一维度的中位数进行左右划分。
            平衡KD树不是搜索最优的。
        搜索KD树：TODO
            算法时间复杂度O(log(N)) N是实例大小.


朴素贝叶斯：TODO



决策树：分类与回归；一般是分类问题。
    三步骤：特征选择，决策树生成，决策树修剪；ID3、C4.5、CART等生成算法。
    特征选择：选择某个特征，按照这个特征分类的各个子集拥有最好的分类结果时候，就应该选这个特征。
        衡量这个标准是信息增益；信息增益比。
        熵：随机变量不确定性的度量。
            有限取值的离散变量的概率分布 P(X=xi) = pi, i=1,2,3,..,n
            熵定义：H(X) = -sigma(pi * log(pi)), pi=0 -> 定义0log0 = 0
            熵依赖p，所以上式写成H(p);熵越大，随机变量不确定性就越大；熵的取值范围：0< =H(p) <= log(n)
            当随机变量只有两个值时；P(X=0) = p, P(X=1) = 1 - p;H(P) = -p * log(p) - (1 - p) * log(1 - p)
                此时p和H(p)的曲线是倒U，当p=0或者1时候H(p)=0，随机变量没有不确定性。p=.5,熵最大。
        条件熵：H(Y|X) = sigma(pi * H(Y|X = xi)),pi = P(X=xi)；pi是X特征取xi时候,样本数/总数；H(Y|X=xi)是X特征取xi时候的样本数中计算熵
        信息增益：特征X使得类Y的信息不确定性减少的程度。
            g(D, A) = H(D) - H(D|A);D是数据集，A是特征。
            熵-条件熵一般称为互信息。
        信息增益比：根据信息增益选取特征，往往偏向于特征值多的特征。
            通过g(D, A) / H(A)(D) 比值来选择特征。H(A)(D)是对特征A计算信息熵，而不是分类了。-sigma(|A特征某个值的样本数|/|总样本| * log (|A特征某个值的样本数|/|总样本|))

    决策树的生成
        ID3:使用信息增益
        C4.5使用信息增益比
        <叶子结点的生成是这一类都是同类别的数据>

    决策树的剪枝

