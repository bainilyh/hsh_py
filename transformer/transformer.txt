Transformer
	1,编码层
		1. 输入维度(批大小, 步长) -> (256, 88)
		2. emb层 -> (256, 88, 512) -> 当作q,k,v
		3. 注意力块 (多轮)
			1. 多头注意力 -> q = k = v 都是 （256, 88, 512）
				1. 全链接 (256, 88, 512) * (512, 512（多头个数*每个注意力维度大小）) -> (256, 88, 512)
				2. 切割多头 (256, 88, 512) -> (256, 88, 8, 64) 使用view(batch_size, num_steps, num_head, n_dk/dq/dv)
				3. 转置 (256,88,8,64) -> (256,8,88,64) 生成 q,k,v
				4. 注意力得分
					4.1 注意力得分  (q, k(2,3维转置)) -> (256, 8, 88, 64) @ (256, 8, 64, 88) -> (256, 8, 88, 88)
					4.2 softmax -> dropout -> (256, 8, 88, 88)
					4.2 融合注意力得分 (256, 8, 88, 64)
				5. 矩阵变换 -> (256, 88, 512)
				6. 全链接 -> dropout -> (256, 88, 512)
				7. 残差 -> layernorm -> (256, 88, 512)

			2. 两层全链接层
				1. 全链接 -> relu -> (256, 88, 512)
				2. 全链接 -> dropout -> (256, 88, 512)
				3. 残差 -> layernorm -> (256, 88, 512)
			3. 输出 (256, 88, 512)

		4. 输出 (256, 88, 512)


	2,解码层
		1. 输入维度
			1. (256, 81) 输入序列
			2. (256, 81, 81) 掩码序列
			3. (256, 77, 512) 解码embedding序列
			4. (256, 1, 77)
		2. 输入序列embedding; 位置编码; dropout; layer_norm -> (256, 81, 512)
		3. 注意力块 (多轮)
			1. 对输入进行一次带掩码的多头注意力计算 q=k=v -> (256, 81, 512)
				1. 全链接 (256, 81, 512) * (512, 512（多头个数*每个注意力维度大小）) -> (256, 81, 512)
				2. 切割多头 (256, 81, 512) -> (256, 81, 8, 64) 使用view(batch_size, num_steps, num_head, n_dk/dq/dv)
				3. 转置 (256,81,8,64) -> (256,8,81,64) 生成 q,k,v
				4. 对掩码序列增加维度 mask.unsqueeze(1) (256, 81, 81) -> (256, 1, 81, 81)
				5. 注意力得分
					5.1 注意力得分  (q, k(2,3维转置)) -> (256, 8, 81, 64) @ (256, 8, 64, 81) -> (256, 8, 81, 81)
					5.2 掩码 (256, 8, 81, 81) -> (256, 8, 81, 81)
					5.3 softmax -> dropout -> (256, 8, 81, 81)
					5.4 融合注意力得分 (256, 8, 81, 64)
				6. 矩阵变换 -> (256, 81, 512)
				7. 全链接 -> dropout -> (256, 81, 512)
				8. 残差 -> layernorm -> (256, 81, 512)
			2. 对输出和encode的结果做多头注意力计算 q=(256, 81, 512) k=v=(256, 77, 512)
				1. 全链接 (256, 81, 512) * (512, 512（多头个数*每个注意力维度大小）) -> q=(256, 81, 512) k=v=(256, 77, 512)
				2. 切割多头 q=(256, 81, 8, 64) k=v=(256, 77, 8, 64)
				3. 转置 q = (256,8,81,64) k=v=(256, 8, 77, 64) 生成 q,k,v
				4. 对掩码序列增加维度 (256, 1, 77) -> (256, 1, 1, 77)
				5. 注意力得分
					5.1 注意力得分  (q, k(2,3维转置)) -> (256, 8, 81, 77)
					5.2 掩码 (256, 8, 81, 77) -> (256, 8, 81, 77)
					5.3 softmax -> dropout -> (256, 8, 81, 77)
					5.4 融合注意力得分 -> (256, 8, 81, 64)
				6. 矩阵变换 -> (256, 81, 512)
				7. 全链接 -> dropout -> (256, 81, 512)
				8. 残差 -> layernorm -> (256, 81, 512)
			3. 两层全链接层 -> (256, 81, 512)
			4. 输出 (256, 81, 512)
	3,过一层emb或全链接 -> (256, 81, 词个数)
	4,输出(256 * 81, 词个数)