 分类
 1.图像分类
    数据集：ImageNet -> 类别 synsets（2万个分类） -> wordnet(层次结构）
 2.图像分类建模
    32*32*3 -> 3072 -> f(x,w) -> 维度为10的向量（多分类问题）
 3.图像分类；目标检测；实例分割（像素级别的分类问题）；
 4.文本例子：垃圾邮件过滤；文档归类；情感分类；
 5.文本分类建模：文本数据 -> 向量形式
    词袋模型（Bag of Words）：1.定义n个词的词典 2.建立n维向量，一维表示一个单词 3.一行文本就就用这个向量表示。
        缺点：没有语序信息了。改进？

线性分类模型
1.二分类问题
    训练集；模型；损失函数
    损失函数：
        0-1损失函数：判断y和y_hat的不同个数（不可求导）
2.多分类问题
    模型：
        一对其余：多个二分类器；3类就是3个二分类模型；有些区域存在模糊地带。
        一对一：两两类别作为新类别；c(c-1)/2个模型；也存在模糊地带。
        argmax：每个类建立个分类器；打分机制；不存在模糊地带。（用的最多的多分类模型）
            损失函数：？
3.Logistic回归；Softmax回归；感知器；支持向量机
    模型的本质都是g(f(x))；区别在损失函数不一样；

4. 交叉熵
    熵：衡量一个随机事件的不确定性；熵越高；随机变量的信息就越多。
    自信息：一个随机事件所包含的信息量;I(x) = -log(p(x);满足可加性。
    熵就是随机变量X的自信息的数学期望。
        H(X) = Ex[I(x)] = Ex[-log(p(x))] = （离散情况）-sigma(p(x) * log(p(x)))
        分布越均衡；熵越大
    熵编码；最优编码是p分布的自信息；
    交叉熵：用概率分布q的最优编码（自信息）对真实分布p的信息进行编码的长度；衡量两个分布的差异。
        H(p,q) = Ep[-log(q(x))] = -sigma(p(x) * log(q(x)))
    KL散度：用概率分布q来近似p时造成的信息损失量。
        KL(p,q) = H(p,q) - H(p) = sigma(p(x) * log(p(x) / q(x)))
    应用到机器学习；以概率角度看分类问题；
        真实分布：Pr(y|x); 预测分布：P(theta)(y|x)
        损失函数；衡量两个分布的差异：minKL(Pr, P(theta)) = min(sigma(pr * log(pr/ p(theta)))) -> KL散度
        由于优化的只有theta,所以优化目标可以简化为最小化-sigma(pr * log(p(theta))) -> 交叉熵
        pr是one-hot值；简化为-sigma(log(p(theta)))，其中选取预测错误的样本进行计算。 -> 负对数似然

5. Logistic回归
g(f(x)) -> f(x)线性函数（判别函数）; g(x)决策函数
把分类问题转换为概率估计问题；损失函数就可以使用交叉熵了；
预测分布P(y=1|x)中的x就是f(x),值域是R；而P是(0,1)，所以得有个非线性函数g，使得R挤压到(0,1)之间。
logistic函数: 1 / (1 + exp(-x)) -> sigmoid型函数；还有哪些函数R->(0,1)?
p(theta)(y=1|x) = 1 / ( 1 + exp(-wx)); 参数w
p(theta)(y=0|x) = 1 - p(theta)(y=1|x)
梯度下降.png

6. softmax回归
函数：exp(xk) / sigma(exp(X))
交叉熵损失：向量表示：-y(T) * log(y(hat)); y是one-hot向量; y(hat)是每个类的预测概率

7. 感知器
函数：x>0 +1; x<=0 -1;
学习目标：y*y(hat) > 0 -> y*(WT*X) > 0
学习方法：错误驱动

TODO

